# v3.md – „Hey Mark!“ App – Finale Komplett-Version (Stand 02.12.2025)
Für Gemini 1.5 Pro / Anti-Gravity – bitte komplett ausführen

## Projektname
„Hey Mark!“ – Die App, die Mark mit seiner eigenen Stimme sagt, wo er zu wenig quatscht.

## Kern-Ziel
Mark hat 140–300 kurze Alltagstipps (je ca. 55–65 s) in einer langen MP3.  
Die App macht automatisch:

1. Intelligentes Segmentieren (keine feste 60 s!)
2. Whisper lokal transkribieren + exakte Timestamps
3. Themen erkennen → maximal 25–30 große Oberthemen (automatisches Merging!)
4. Prozent-Abdeckung in schickem Glassmorphism-Dashboard
5. Grok generiert exakt 4 neue 60-Sekunden-Vorschläge für die schwächsten Themen – in Marks exaktem Stil und exakt gleicher Länge
6. ElevenLabs klont Marks Stimme → Button „KI-Mark vorlesen“ spielt die Vorschläge mit Marks Stimme ab

## Tech-Stack (fix)
- Frontend: Next.js 14 (App Router) + TypeScript + Tailwind CSS
- Design: Dark Mode only + Glassmorphism + backdrop-blur-xl + cyan Neon-Hover
- Backend: Python 3.12 + FastAPI (uvicorn)
- Transkription: openai-whisper „large-v3“ lokal
- Split: pydub + ffmpeg-python (nach Whisper-Timestamps + Stille-Erkennung)
- Datenbank: MongoDB 7 (Docker) – Collections: `clips`, `uploads`, `merged_themes`, `style_cache`
- KI-Analyse & Stil: Grok-API (xAI)
- Voice-Cloning & TTS: ElevenLabs API (Instant Voice Cloning + TTS)
- Audio-Cleanup: webrtcvad + RNNoise (Adobe-Podcast-Style)
- Hosting: Hetzner Cloud – Docker-Compose – Port 3000 extern

## Datenmodell (MongoDB)

```json
// clips
{
  "_id": ObjectId,
  "upload_id": UUID,
  "segment_nr": 42,
  "start_sec": 2584.1,
  "end_sec": 2644.7,
  "duration_sec": 60.6,
  "text": "Also pass auf, wenn du Socken richtig putzen willst...",
  "raw_topic": "Sockenpflege",
  "final_topic": "Wäsche & Pflege",      // nach Merging
  "style": {
    "filler_words": "äh, nein nein, pass auf, also",
    "pace": "schnell",
    "tone": "leicht nörgelnd"
  },
  "clip_path": "/uploads/clips/xxx_042.mp3",
  "created_at": ISODate
}
Kompletter Workflow

Upload → /upload (multipart, max 2 GB)
FastAPI speichert → Whisper large-v3 transkribiert (word_timestamps=True)
Segmente erkennen → >45 s & <90 s + Stille >1.2 s oder Jingle-Peak
Jeder Clip wird mit pydub/ffmpeg exakt ausgeschnitten + gereinigt (RNNoise)
Text → Grok (Topic + Style-Analyse)
Roh-Topics → zweiter Grok-Run: „Merge alles >60 % ähnlich → max. 30 finale Oberthemen“
Ergebnis in merged_themes → Dashboard zeigt nur noch diese 30
Button „4 neue Vorschläge“ → Grok bekommt Style-Cache + schwächste 4 Themen → siehe Prompt unten
Rückgabe → 4 Texte mit exakt 138–142 Wörtern → Word-Check-Schleife → ElevenLabs TTS mit geklonter Voice-ID → MP3 abspielen

Exakter Grok-Prompt für die 4 Vorschläge (muss 1:1 so benutzt werden)
textDu bist KI-Mark – sprich EXAKT wie Mark (Füllwörter, Tempo, leichter Meckerton).
Style-Info aus 140+ Clips:
{{style.filler_words}} – {{style.pace}} – {{style.tone}}

Generiere genau 4 neue 60-Sekunden-Tipps zu diesen unterrepräsentierten Themen:
1. {{topic1}} – nur {{percent1}} %
2. {{topic2}} – nur {{percent2}} %
3. {{topic3}} – nur {{percent3}} %
4. {{topic4}} – nur {{percent4}} %

Jeder Tipp muss:
- exakt 138–142 Wörter haben (bei Marks Tempo = 60 Sekunden)
- mit „Hey Leute…“, „Also pass auf…“ oder ähnlich anfangen
- mit einem klaren Abschluss enden

Antworte NUR mit diesem JSON (kein weiterer Text!):

{
  "vorschlag_1": "kompletter Text (138–142 Wörter)",
  "vorschlag_2": "kompletter Text (138–142 Wörter)",
  "vorschlag_3": "kompletter Text (138–142 Wörter)",
  "vorschlag_4": "kompletter Text (138–142 Wörter)"
}
ElevenLabs Integration (einmalig + TTS)

Einmalig: 3–5 saubere Clips hochladen → Instant Voice Cloning → Voice-ID in .env speichern
TTS-Call (Python):

Pythonurl = f"https://api.elevenlabs.io/v1/text-to-speech/{ELEVEN_VOICE_ID}/stream"
payload = { "text": text, "voice_settings": { "stability": 0.75, "similarity_boost": 0.9 } }
→ MP3 direkt im Browser streamen oder downloaden
Frontend – Glassmorphism Specs

Background: schwarzes Noise-Gradient
Glass-Cards:

CSS.glass {
  @apply bg-white/5 backdrop-blur-xl border border-white/10 rounded-2xl 
         shadow-2xl transition-all duration-300 
         hover:shadow-cyan-500/50 hover:border-cyan-500/30 hover:scale-105;
}

Jede Topic-Card: Thema groß, Prozent-Ring, Minuten, Hover → Modal mit Player + Transkript
Vorschläge-Bereich: 4 Karten + „KI-Mark vorlesen“-Button (spielt direkt ab)
Unter jedem Vorschlag: „140 Wörter – 60 Sekunden“

Docker-Compose (komplett)
YAMLversion: "3.9"
services:
  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://backend:8000

  backend:
    build: ./backend
    volumes:
      - ./uploads:/app/uploads
    depends_on:
      - mongo
    environment:
      - GROK_API_KEY=xxx
      - ELEVEN_API_KEY=xxx
      - ELEVEN_VOICE_ID=xxx
      - MONGODB_URL=mongodb://mongo:27017/heymark

  mongo:
    image: mongo:7
    restart: unless-stopped
    volumes:
      - mongo-data:/data/db

volumes:
  mongo-data:
Was Gemini jetzt genau bauen soll
Erstelle mir den kompletten lauffähigen Code:

Ordnerstruktur frontend + backend
Next.js 14 App mit Upload-Seite, Dashboard, Glassmorphism-Komponenten, Recharts
FastAPI mit allen Routen (upload, segments, merge-themes, suggestions, tts)
Whisper + ffmpeg + RNNoise + ElevenLabs Integration
Automatisches Thema-Merging + Style-Cache
Word-Count-Check-Schleife für exakt 138–142 Wörter
docker-compose.yml + Dockerfile + .env.example + README.md (Hetzner-Startanleitung)

Nach docker compose up --build muss alles auf http://IP:3000 laufen und sofort einsatzbereit sein.
Das ist die finale, runde Version v3 – jetzt kann nichts mehr schiefgehen.
Viel Spaß, Mic & Mark! ❤